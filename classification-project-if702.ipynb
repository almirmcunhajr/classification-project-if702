{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import seed\n",
    "seed(1)\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "from tensorboard.plugins.hparams import api as hp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_data = pd.read_csv('BASE-PREPROCESSED(TRAIN).gz', sep=\"\\t\")\n",
    "validation_data = pd.read_csv('BASE-PREPROCESSED(VALIDACAO).gz', sep=\"\\t\")\n",
    "test_data = pd.read_csv('BASE-PREPROCESSED(TESTE).gz', sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parameter_nomalization(data):\n",
    "  class_min = data.loc[data['ALVO'] == 0]\n",
    "  times = int((data.shape[0] - class_min.shape[0]) / class_min.shape[0])\n",
    "  for i in range(times):\n",
    "      data = pd.concat([data, class_min], ignore_index=False, verify_integrity=False, sort=False)\n",
    "\n",
    "  return data\n",
    "\n",
    "train_data = parameter_nomalization(train_data)\n",
    "validation_data = parameter_nomalization(validation_data)\n",
    "test_data = parameter_nomalization(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data.drop(['PROPHET_NORM_FEATURES'], axis=1)\n",
    "validation_data = validation_data.drop(['PROPHET_NORM_FEATURES'], axis=1)\n",
    "test_data = test_data.drop(['PROPHET_NORM_FEATURES'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data.drop(['CLASSE_SOCIAL_PCNC_PCM_rv_1', 'CLASSE_SOCIAL_PCNC_PCM_rv_0', 'CLASSE_SOCIAL_PCNC_PCM_rv_2'], axis=1)\n",
    "y_train = train_data['ALVO']\n",
    "x_train = train_data.drop(['ALVO', 'PROPHET_LABEL', 'NEURO_LABEL'], axis=1)\n",
    "x_train = x_train.to_numpy()\n",
    "y_train = y_train.to_numpy()\n",
    "\n",
    "y_val = validation_data['ALVO']\n",
    "x_val = validation_data.drop(['ALVO', 'PROPHET_LABEL', 'NEURO_LABEL'], axis=1)\n",
    "x_val = x_val.to_numpy()\n",
    "y_val = y_val.to_numpy()\n",
    "\n",
    "y_test = test_data['ALVO']\n",
    "x_test = test_data.drop(['ALVO', 'PROPHET_LABEL', 'NEURO_LABEL'], axis=1)\n",
    "x_test = x_test.to_numpy()\n",
    "y_test = y_test.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "\n",
    "from scipy.stats import ks_2samp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HP_NUM_UNITS1 = hp.HParam('num_units 1', hp.Discrete([32,64,128,256]))\n",
    "HP_NUM_UNITS2 = hp.HParam('num_units 2', hp.Discrete([32,64,128,256]))\n",
    "HP_NUM_UNITS3 = hp.HParam('num_units 3', hp.Discrete([32,64,128,256]))\n",
    "HP_OPTIMIZER = hp.HParam('optimizer', hp.Discrete(['adam','RMSprop']))\n",
    "HP_ACTIVATION = hp.HParam('activation', hp.Discrete(['relu', 'tanh']))\n",
    "HP_L2 = hp.HParam('l2 regularizer', hp.RealInterval(.001,.01))\n",
    "METRIC_ACCURACY = 'accuracy'\n",
    "METRIC_RECALL = 'recall'\n",
    "METRIC_PRECISION = 'precision'\n",
    "METRIC_F1 = 'f1'\n",
    "METRIC_AUROC = 'auroc'\n",
    "METRIC_AUPR = 'aupr'\n",
    "METRIC_KS = 'ks'\n",
    "\n",
    "with tf.summary.create_file_writer('logs/results').as_default():\n",
    "  hp.hparams_config(\n",
    "    hparams=[HP_NUM_UNITS1,HP_NUM_UNITS2, HP_NUM_UNITS3, HP_L2 ,HP_OPTIMIZER, HP_ACTIVATION],\n",
    "    metrics=[hp.Metric(METRIC_ACCURACY, display_name='Accuracy')],\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aux(s, g):\n",
    "    if s in g.index:\n",
    "        return g[s]\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def kstest(x, y):\n",
    "    threshold = 0\n",
    "    df = pd.DataFrame({'ALVO': y, 'x': x})\n",
    "    n0, n1 = df['ALVO'].value_counts().values\n",
    "    data0, data1 = [], []\n",
    "    while threshold < 1.001:\n",
    "        g = df[df['x'] <= threshold]['ALVO'].value_counts()\n",
    "        data0.append(g.get(0, 0) / n0)\n",
    "        data1.append(g.get(1, 0) / n1)\n",
    "        threshold += 0.0001\n",
    "    return data0, data1\n",
    "\n",
    "def compute_performance_metrics(y, y_pred_class, y_pred_scores=None):\n",
    "    accuracy = accuracy_score(y, y_pred_class)\n",
    "    recall = recall_score(y, y_pred_class)\n",
    "    precision = precision_score(y, y_pred_class)\n",
    "    f1 = f1_score(y, y_pred_class)\n",
    "    performance_metrics = (accuracy, recall, precision, f1)\n",
    "    if y_pred_scores is not None:\n",
    "        auroc = roc_auc_score(y, y_pred_scores)\n",
    "        aupr = average_precision_score(y, y_pred_scores)\n",
    "        data0, data1 = kstest(y_pred_scores,y)\n",
    "        ks = ks_2samp(data0,data1)\n",
    "        performance_metrics = performance_metrics + (auroc, aupr, ks)\n",
    "    return performance_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_model(hparams):\n",
    "  model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(hparams[HP_NUM_UNITS1], kernel_regularizer=tf.keras.regularizers.l2(0.001), activation=hparams[HP_ACTIVATION]),\n",
    "    tf.keras.layers.Dense(hparams[HP_NUM_UNITS2], kernel_regularizer=tf.keras.regularizers.l2(0.001), activation=hparams[HP_ACTIVATION]),\n",
    "    tf.keras.layers.Dense(hparams[HP_NUM_UNITS3], kernel_regularizer=tf.keras.regularizers.l2(0.001), activation=hparams[HP_ACTIVATION]),\n",
    "    tf.keras.layers.Dense(1, activation=hparams[HP_ACTIVATION])\n",
    "  ])\n",
    "\n",
    "  model.compile(optimizer=hparams[HP_OPTIMIZER],\n",
    "                loss='mean_squared_error',\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "  es = EarlyStopping(monitor='val_accuracy', mode='auto', verbose=1, min_delta=0.001, patience=10)\n",
    "\n",
    "  print(\"Training...\")\n",
    "  model.fit(x_train,\n",
    "            y_train,\n",
    "            validation_data=(x_val, y_val),\n",
    "            epochs=500,\n",
    "            batch_size=131072,\n",
    "            callbacks=[es])\n",
    "\n",
    "  print(\"Evaluating...\")\n",
    "  y_pred_scores = model.predict(x_test)\n",
    "  y_pred_class = model.predict_classes(x_test, verbose=0)\n",
    "  accuracy, recall, precision, f1, auroc, aupr, ks = compute_performance_metrics(y_test, y_pred_class, y_pred_scores)\n",
    "  \n",
    "  print(\"Done!\")\n",
    "\n",
    "  return accuracy, recall, precision, f1, auroc, aupr, ks[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(run_dir, hparams):\n",
    "  with tf.summary.create_file_writer(run_dir).as_default():\n",
    "    hp.hparams(hparams)\n",
    "    accuracy, recall, precision, f1, auroc, aupr, ks = train_test_model(hparams)\n",
    "    tf.summary.scalar(METRIC_ACCURACY, accuracy, step=1)\n",
    "    tf.summary.scalar(METRIC_RECALL, recall, step=1)\n",
    "    tf.summary.scalar(METRIC_PRECISION, precision, step=1)\n",
    "    tf.summary.scalar(METRIC_F1, f1, step=1)\n",
    "    tf.summary.scalar(METRIC_AUROC, auroc, step=1)\n",
    "    tf.summary.scalar(METRIC_AUPR, aupr, step=1)\n",
    "    tf.summary.scalar(METRIC_KS, ks, step=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session_num = 0\n",
    "for num_units1 in HP_NUM_UNITS1.domain.values:\n",
    "  for num_units2 in HP_NUM_UNITS2.domain.values:\n",
    "    for num_units3 in HP_NUM_UNITS3.domain.values:\n",
    "      for activation in (HP_ACTIVATION.domain.values):\n",
    "        for l2 in (HP_L2.domain.min_value, HP_L2.domain.max_value):\n",
    "          for optimizer in HP_OPTIMIZER.domain.values:\n",
    "            hparams = {\n",
    "                HP_NUM_UNITS1: num_units1,\n",
    "                HP_NUM_UNITS2: num_units2,\n",
    "                HP_NUM_UNITS3: num_units3,\n",
    "                HP_ACTIVATION: activation,\n",
    "                HP_L2: l2,\n",
    "                HP_OPTIMIZER: optimizer\n",
    "            }\n",
    "            run_name = \"run-%d\" % session_num\n",
    "\n",
    "            print('--- Starting trial: %s' % run_name)\n",
    "            print({h.name: hparams[h] for h in hparams})\n",
    "\n",
    "            run('logs/results/' + run_name, hparams)\n",
    "            session_num += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "classification-project-if702",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
